{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aff943aa-e106-4537-b986-bdbb4fd73866",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "* Intent Classification 테스크를 위한 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a222b2c-6fea-4fde-9072-731511980d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "from src.dataset import Preprocessing\n",
    "from src.model import EpochLogger, MakeEmbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2918c38-0410-44f8-9626-3db17720b8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\it강남-e강의장(강사용)\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.1.96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\it강남-e강의장(강사용)\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1598b236-d490-4cda-a581-a531bb83149a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-crf in c:\\users\\it강남-e강의장(강사용)\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (0.7.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\it강남-e강의장(강사용)\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "162d9d2b-c132-49f0-ae19-3951f591a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDataset:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.intent_label_dir = \"./data/dataset/intent_label.json\"\n",
    "        self.intent_data_dir = \"./data/dataset/intent_data.csv\"\n",
    "        \n",
    "        self.intent_label = self.load_intent_label()\n",
    "        self.prep = Preprocessing()\n",
    "\n",
    "    def load_intent_label(self):\n",
    "        f = open(self.intent_label_dir, encoding=\"UTF-8\") \n",
    "        intent_label = json.loads(f.read())\n",
    "        self.intents = list(intent_label.keys())\n",
    "        return intent_label\n",
    "    \n",
    "    def tokenize(self, sentence):\n",
    "        return sentence.split()\n",
    "    \n",
    "    def tokenize_dataset(self, dataset):\n",
    "        token_dataset = []\n",
    "        for data in dataset:\n",
    "            token_dataset.append(self.tokenize(data))\n",
    "        return token_dataset\n",
    "\n",
    "    def make_intent_dataset(self, embed): # intent 분류를 위한 Dataset 생성\n",
    "        intent_dataset = pd.read_csv(self.intent_data_dir) # 데이터를 로드\n",
    "\n",
    "        labels = [self.intent_label[label] for label in intent_dataset[\"label\"].to_list()] # label\n",
    "        \n",
    "        # 사용자 발화 : 사용자가 챗봇과 커뮤니케이션하기 위해 내뱉는 말 또는 텍스트등을 의미\n",
    "        intent_querys = self.tokenize_dataset(intent_dataset[\"question\"].tolist()) # 사용자 발화 tokenize\n",
    "        \n",
    "        dataset = list(zip(intent_querys, labels)) # (사용자 발화, intent) 형태로 가공\n",
    "        intent_train_dataset, intent_test_dataset = self.word2idx_dataset(dataset, embed) # word2index\n",
    "        return intent_train_dataset, intent_test_dataset\n",
    "    \n",
    "    def word2idx_dataset(self, dataset ,embed, train_ratio = 0.8):\n",
    "        embed_dataset = []\n",
    "        question_list, label_list = [], []\n",
    "        flag = True\n",
    "        random.shuffle(dataset) # 훈련용과 검증용으로 나눌 때 intent 편형이 나타나지 않도록 데이터를 셔플\n",
    "        for query, label in dataset :\n",
    "            q_vec = embed.query2idx(query) # 사용자 발화 index 형태로 변환\n",
    "            q_vec = self.prep.pad_idx_sequencing(q_vec) # 사용자 발화 최대길이까지 padding\n",
    "\n",
    "            question_list.append(torch.tensor([q_vec]))\n",
    "            label_list.append(torch.tensor([label]))\n",
    "\n",
    "        x = torch.cat(question_list)\n",
    "        y = torch.cat(label_list)\n",
    "\n",
    "        # 학습용과 검증용으로 나누기\n",
    "        x_len = x.size()[0]\n",
    "        y_len = y.size()[0]\n",
    "        if(x_len == y_len):\n",
    "            train_size = int(x_len*train_ratio)\n",
    "            \n",
    "            train_x = x[:train_size]\n",
    "            train_y = y[:train_size]\n",
    "\n",
    "            test_x = x[train_size+1:]\n",
    "            test_y = y[train_size+1:]\n",
    "            # TensorDataset으로 감싸기\n",
    "            # 파이토치의 TensorDatset은 tensor를 감싸는 Dataset\n",
    "            # 인덱싱 방식과 길이를 정의함으로써 tensor의 첫 번째 차원을 따라 반복, 인덱스, 슬라이스를 위한 방법을 제공\n",
    "            # 학습할 때 동일한 라이엔서 독립 변수와 종속 변수에 쉽게 접근할 수 있음\n",
    "            train_dataset = TensorDataset(train_x,train_y)\n",
    "            test_dataset = TensorDataset(test_x,test_y)\n",
    "            \n",
    "            return train_dataset, test_dataset\n",
    "            \n",
    "        else:\n",
    "            print(\"ERROR x!=y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc736b3a-e183-4fbc-806e-eae7900045bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MakeDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2d4fb77-f91b-4a39-bfd1-1a9ad4efebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_dataset = pd.read_csv(dataset.intent_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "691fe915-00d3-4c2b-b309-a9b1503396fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>야 먼지 알려주겠니</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>아니 먼지 정보 알려주세요</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>그 때 미세먼지 어떨까</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>그 때 먼지 좋으려나</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>미세먼지 어떨 것 같은데</td>\n",
       "      <td>dust</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         question label\n",
       "0      야 먼지 알려주겠니  dust\n",
       "1  아니 먼지 정보 알려주세요  dust\n",
       "2    그 때 미세먼지 어떨까  dust\n",
       "3     그 때 먼지 좋으려나  dust\n",
       "4   미세먼지 어떨 것 같은데  dust"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cf5153f-2afd-4f81-9afb-9504884848fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dust</th>\n",
       "      <td>4997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restaurant</th>\n",
       "      <td>4997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel</th>\n",
       "      <td>4999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weather</th>\n",
       "      <td>4999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            question\n",
       "label               \n",
       "dust            4997\n",
       "restaurant      4997\n",
       "travel          4999\n",
       "weather         4999"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_dataset.groupby(['label']).count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34821ce9-e310-4086-94d1-22d47f13c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = MakeEmbed()\n",
    "embed.load_word2vec()\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "intent_train_dataset, intent_test_dataset = dataset.make_intent_dataset(embed)\n",
    "\n",
    "train_dataloader = DataLoader(intent_train_dataset, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "test_dataloader = DataLoader(intent_test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d00ebd3-13e9-4e35-a1a7-f470fd584359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 573, 1114,    2,  ...,    0,    0,    0],\n",
       "         [ 901,  907,   14,  ...,    0,    0,    0],\n",
       "         [ 303,  205,   14,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 176,   47,   99,  ...,    0,    0,    0],\n",
       "         [ 243,   21,    2,  ...,    0,    0,    0],\n",
       "         [  47,   99,  614,  ...,    0,    0,    0]]),\n",
       " tensor([0, 1, 1,  ..., 1, 0, 1]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intent_train_dataset.tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b6a7bb-42bc-487c-9b97-fabcc8667c57",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks for Sentence Classification\n",
    "### tensorflow code : https://github.com/SeonbeomKim/TensorFlow-TextCNN/blob/master/TextCNN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d164c2b9-8470-43ca-984f-4e00f8fb2bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class textCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, w2v, dim, kernels, dropout, num_class):\n",
    "        super(textCNN, self).__init__()\n",
    "        # Word2vec으로 미리 학습해둔 임베딩 적용\n",
    "        vocab_size = w2v.size()[0]\n",
    "        emb_dim = w2v.size()[1]\n",
    "        self.embed = nn.Embedding(vocab_size+2, emb_dim)\n",
    "        self.embed.weight[2:].data.copy_(w2v)\n",
    "        # self.embed.weight.requires_grad = False # 임베딩 레이어 학습 유무\n",
    "        \n",
    "        # 윈도우 사이즈가 다른 각각의 conv layer 를 nn.ModuleList로 저장\n",
    "        # nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, dim, (w, emb_dim)) for w in kernels])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(len(kernels)*dim, num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb_x = self.embed(x)\n",
    "        emb_x = emb_x.unsqueeze(1)\n",
    "\n",
    "        con_x = [conv(emb_x) for conv in self.convs] # 각 사이즈 별 결과를 list로 저장\n",
    "        # [(out_channels, conv결과 길이), ...]\n",
    "\n",
    "        pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x] # 각 사이즈별 max_pool결과 저장\n",
    "        # [(256, 1), ...]\n",
    "\n",
    "        fc_x = torch.cat(pool_x, dim=1) # concat 하여 fc layer의 입력 형태로 만듬\n",
    "\n",
    "        fc_x = fc_x.squeeze(-1) # 차원 맞추기\n",
    "        fc_x = self.dropout(fc_x) \n",
    "        logit = self.fc(fc_x)\n",
    "        return logit\n",
    "\n",
    "# 모델의 가중치 저장을 위한 코드\n",
    "def save(model, save_dir, save_prefix, epoch):\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_prefix = os.path.join(save_dir, save_prefix)\n",
    "    save_path = '{}_steps_{}.pt'.format(save_prefix, epoch)\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9057b5ab-4245-48fc-a687-dd384489de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = embed.word2vec.wv.vectors\n",
    "weights = torch.FloatTensor(weights)\n",
    "\n",
    "num_class = len(dataset.intent_label) \n",
    "model = textCNN(weights, 256, [3,4,5], 0.5, num_class)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d32acf21-e83b-48d2-9dc5-9a15b0a7b352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textCNN(\n",
       "  (embed): Embedding(1481, 300)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))\n",
       "    (1): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))\n",
       "    (2): Conv2d(1, 256, kernel_size=(5, 300), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa46bc7-5793-4d77-abdd-0951631af3fa",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b694b245-53ef-4017-9d22-d424a83ec07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|█| 125/125 [00:25<00:00,  4.88batch/s, accuracy=98.3471\n",
      "Epoch 0: 100%|█| 32/32 [00:01<00:00, 27.10batch/s, accuracy=99.2, los\n",
      "Epoch 1: 100%|█| 125/125 [00:27<00:00,  4.58batch/s, accuracy=100.0, \n",
      "Epoch 1: 100%|█| 32/32 [00:01<00:00, 24.64batch/s, accuracy=99, loss=\n",
      "Epoch 2: 100%|█| 125/125 [00:27<00:00,  4.59batch/s, accuracy=100.0, \n",
      "Epoch 2: 100%|█| 32/32 [00:01<00:00, 26.30batch/s, accuracy=99.4, los\n",
      "Epoch 3: 100%|█| 125/125 [00:26<00:00,  4.72batch/s, accuracy=99.1735\n",
      "Epoch 3: 100%|█| 32/32 [00:01<00:00, 26.34batch/s, accuracy=99.1, los\n",
      "Epoch 4: 100%|█| 125/125 [00:22<00:00,  5.58batch/s, accuracy=99.1735\n",
      "Epoch 4: 100%|█| 32/32 [00:01<00:00, 26.05batch/s, accuracy=99.2, los\n",
      "Epoch 5: 100%|█| 125/125 [00:20<00:00,  5.99batch/s, accuracy=99.1735\n",
      "Epoch 5: 100%|█| 32/32 [00:01<00:00, 25.53batch/s, accuracy=99, loss=\n",
      "Epoch 6: 100%|█| 125/125 [00:22<00:00,  5.65batch/s, accuracy=100.0, \n",
      "Epoch 6: 100%|█| 32/32 [00:01<00:00, 18.86batch/s, accuracy=99.3, los\n",
      "Epoch 7: 100%|█| 125/125 [00:20<00:00,  6.19batch/s, accuracy=100.0, \n",
      "Epoch 7: 100%|█| 32/32 [00:01<00:00, 24.89batch/s, accuracy=99.4, los\n",
      "Epoch 8: 100%|█| 125/125 [00:19<00:00,  6.51batch/s, accuracy=100.0, \n",
      "Epoch 8: 100%|█| 32/32 [00:01<00:00, 24.47batch/s, accuracy=99.2, los\n",
      "Epoch 9: 100%|█| 125/125 [00:19<00:00,  6.47batch/s, accuracy=100.0, \n",
      "Epoch 9: 100%|█| 32/32 [00:01<00:00, 24.56batch/s, accuracy=99.1, los"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epoch = 10\n",
    "prev_acc = 0\n",
    "save_dir = \"./data/pretraining/1_intent_clsf_model/\"\n",
    "save_prefix = \"intent_clsf\"\n",
    "for i in range(epoch):\n",
    "    steps = 0\n",
    "    model.train() \n",
    "    #for data in train_dataloader:\n",
    "    with tqdm(train_dataloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {i}\")\n",
    "            x = data[0]\n",
    "            target = data[1]\n",
    "            logit = model.forward(x)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(logit, target) # loass function\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            accuracy = 100.0 * corrects/x.size()[0]\n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy= accuracy.numpy())\n",
    "    \n",
    "    model.eval() # weight 업데이트 금지\n",
    "    steps = 0\n",
    "    accuarcy_list = []\n",
    "    #for data in test_dataloader:\n",
    "    with tqdm(test_dataloader, unit=\"batch\") as tepoch:\n",
    "        for data in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {i}\")\n",
    "            x = data[0]\n",
    "            target = data[1]\n",
    "\n",
    "            logit = model.forward(x)\n",
    "            loss = F.cross_entropy(logit, target)\n",
    "            corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            accuracy = 100.0 * corrects/x.size()[0]\n",
    "            accuarcy_list.append(accuracy.tolist())\n",
    "            \n",
    "            tepoch.set_postfix(loss=loss.item(), accuracy= sum(accuarcy_list)/len(accuarcy_list))\n",
    "    \n",
    "    acc = sum(accuarcy_list)/len(accuarcy_list)\n",
    "    if(acc>prev_acc):\n",
    "        prev_acc = acc\n",
    "        save(model, save_dir, save_prefix+\"_\"+str(round(acc,3)), i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4903a9aa-7ea3-4941-9fea-9137324906c5",
   "metadata": {},
   "source": [
    "# Load & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cb1e360-e28f-4b9b-b371-410b051dfb9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textCNN(\n",
       "  (embed): Embedding(1481, 300)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))\n",
       "    (1): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))\n",
       "    (2): Conv2d(1, 256, kernel_size=(5, 300), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./data/pretraining/save/1_intent_clsf_model/intent_clsf_97.217_steps_33.pt\"))\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ca892129-b8d3-4ab7-919e-49e309bea681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발화 : 제주도 오늘 날씨 알려줘\n",
      "의도 : weather\n",
      "Wall time: 3.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "q = \"제주도 오늘 날씨 알려줘\"\n",
    "\n",
    "x = dataset.prep.pad_idx_sequencing(embed.query2idx(dataset.tokenize(q)))\n",
    "\n",
    "x = torch.tensor(x)\n",
    "f = model(x.unsqueeze(0))\n",
    "\n",
    "intent = dataset.intents[torch.argmax(f).tolist()]\n",
    "\n",
    "print(\"발화 : \" + q)\n",
    "print(\"의도 : \" + intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3044d3-aa8d-4459-8d63-4046915a4d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
